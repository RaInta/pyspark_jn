{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some General Spark Optimization Comments\n",
    "\n",
    "\n",
    "\n",
    "### Various caching strategies\n",
    "\n",
    "Persist: marks a reqeust to cache the query on the Dataset the next time an action is performed (default storage level: `MEMORY_AND_DISK`)\n",
    "\n",
    "Caching: same as `.persist()`, but the _only_ storage type is `MEMORY_ONLY`.\n",
    "\n",
    "**What's the difference?**\n",
    "\n",
    "Both deserialize the RDD in the JVM in memory. For `MEMORY_ONLY`, if it doesn't fit, partitions have to be re-constituted as-needed. `MEMORY_AND_DISK`, the rest is spilled to disk and read from there. They both use up a lot of space; the former has lower latency, but may require more memory management (garbage collector). Optimization depends on the available RAM on executors.\n",
    "\n",
    "Checkpointing: Make a request to produce an internal representation (`internalRDD`) of the data lineage and make a reliable copy.\n",
    "\n",
    "\n",
    "### Catalyst\n",
    "\n",
    "Catalyst is a framework to generate a dataflow graph of expressions and relational operators. The graph is a tree structure (`TreeNode`) that contains `Expressions` and `QueryPlans`.\n",
    "\n",
    "Catalyst Optimizer, `Optimizer`, defines the rules to take a structured query and generate an optimized logical plan. You can access the optimized logical plan with `Dataset.explain(extended=True)`  (SQL: `EPLAIN EXTENDED`)\n",
    "\n",
    "\n",
    "### Type Safety and DataFrames, SQL and Datasets\n",
    "\n",
    "\n",
    "This book (nice Spark SQL reference): <a href=\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Dataset.html\">\"The Internals of Spark SQL,\" by Jacek Laskowski</a>\n",
    "\n",
    "\n",
    "> The Dataset API offers declarative and type-safe operators that makes for an improved experience for data processing (comparing to DataFrames that were a set of index- or column name-based Rows).\n",
    "\n",
    "Unlike DataFrames and SQL queries, Datasets are strongly typed and consequently have checks at compile time.\n",
    "\n",
    "### Tungsten\n",
    "\n",
    "Tungsten transforms Spark operations into (native) runtime virtual functions, focusing on computational efficiency and fine-grained memory management.\n",
    "\n",
    "\n",
    "\n",
    "### Optimized Joins\n",
    "\n",
    "\n",
    "Joins will be automatically broadcast if one table/DF is small enough to fit into memory. The default is 10 MB\n",
    "\n",
    "Maximum size (in bytes) for a table that will be broadcast to all worker nodes when performing a join. \n",
    "\n",
    "If the size of the statistics of the logical plan of a table is at most the setting, the DataFrame is broadcast for join.\n",
    "\n",
    "Negative values or 0 disable broadcasting.\n",
    "\n",
    "Use `SQLConf.autoBroadcastJoinThreshold` method to access the current value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
